import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy import FormRequest
from scrapy.exceptions import CloseSpider
import yaml
import urllib.parse
import json
import re
import html
import os
from .utils import extract_links, get_browser_headers, get_md5


class UniversalSpider(scrapy.Spider):
    name = "universal_spider"

    def __init__(self, site_cfg, keyword, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.site_cfg = site_cfg
        self.keyword = keyword
        # ä»site_cfgä¸­è·å–task_id
        self.task_id = site_cfg.get('task_id')
        self.context = {"host": site_cfg.get('host'), "keyword": keyword}
        self.base_headers = get_browser_headers(site_cfg.get('host'))

        # æŒ‡çº¹å»é‡é›†åˆä¸é”™è¯¯ç»Ÿè®¡
        self.seen_resources = set()
        self.error_count = 0
        self.max_errors = 10  # è¿ç»­é”™è¯¯ç†”æ–­é˜ˆå€¼

    def start_requests(self):
        workflow = self.site_cfg.get('workflow', [])
        if workflow:
            yield from self.run_workflow_step(0)
        else:
            yield from self.execute_search()

    def run_workflow_step(self, index):
        step = self.site_cfg['workflow'][index]
        url = self.render_template(step['url'])
        self.logger.info(f"ğŸ”„ å·¥ä½œæµæ­¥éª¤ {index + 1}: {url}")

        meta = {'handle_httpstatus_list': [403, 429]}
        yield scrapy.Request(
            url,
            headers=self.base_headers,
            callback=self.parse_workflow,
            meta=meta,
            cb_kwargs={'step_index': index},
            dont_filter=True
        )

    def parse_workflow(self, response, step_index):
        if response.status in [403, 429]:
            self.logger.warning(f"âš ï¸ å·¥ä½œæµå—é™ ({response.status})ï¼Œç«™ç‚¹: {self.site_cfg['name']}")
            return

        step = self.site_cfg['workflow'][step_index]
        for var_name, rule in step.get('extract', {}).items():
            val = None
            if rule.startswith('xpath:'):
                val = response.xpath(rule[6:]).get()
            elif rule.startswith('regex:'):
                match = re.search(rule[6:], response.text)
                val = match.group(1) if match else None
            if val: self.context[var_name] = val

        if step_index + 1 < len(self.site_cfg['workflow']):
            yield from self.run_workflow_step(step_index + 1)
        else:
            yield from self.execute_search()

    def execute_search(self):
        cfg = self.site_cfg
        url = self.render_template(cfg['start_url'])
        method = cfg.get('method', 'GET').upper()

        meta = {'handle_httpstatus_list': [403, 422, 429]}
        if cfg.get('handle_redirect'):
            meta['handle_redirect'] = True

        headers = self.base_headers.copy()
        if 'headers' in cfg:
            headers.update(cfg['headers'])

        if method == 'POST':
            raw_payload = cfg.get('payload', {}).copy()
            processed_payload = {}
            for k, v in raw_payload.items():
                processed_payload[k] = self.render_template(v) if isinstance(v, str) else v

            processed_payload[cfg.get('kw_field', 'keyboard')] = self.keyword

            if headers.get('Content-Type') == 'application/json':
                yield scrapy.Request(url, method='POST', body=json.dumps(processed_payload),
                                     headers=headers, callback=self.parse_result, meta=meta)
            else:
                yield FormRequest(url, formdata=processed_payload, headers=headers,
                                  callback=self.parse_result, meta=meta)
        else:
            yield scrapy.Request(url, headers=headers, callback=self.parse_result, meta=meta)

    def parse_result(self, response):
        cfg = self.site_cfg
        has_detail = cfg.get('has_detail', True)

        if response.status in [403, 422, 429]:
            self.error_count += 1
            if self.error_count >= self.max_errors:
                raise CloseSpider(f"ç«™ç‚¹ {cfg['name']} è¿ç»­æŠ¥é”™ï¼Œè§¦å‘è‡ªåŠ¨ç†”æ–­")
            return

        self.error_count = 0
        mode = cfg.get('parse_mode', 'html')
        detail_meta = {'handle_httpstatus_list': [403], 'referer_url': response.url}

        if mode == 'json':
            try:
                data = json.loads(response.text)
                items_path = cfg.get('json_items_path', 'data')
                items = data
                for key in items_path.split('.'):
                    if isinstance(items, dict): items = items.get(key, [])

                if not isinstance(items, list): items = [items]

                for item in items:
                    title = self.get_json_value(item, cfg.get('json_title_path', 'name'))
                    # DEBUGæ¨¡å¼ä¸‹æŸ¥çœ‹è·³è¿‡çš„æ ‡é¢˜
                    if not title or (self.keyword and self.keyword.lower() not in str(title).lower()):
                        self.logger.debug(f"è·³è¿‡æ ‡é¢˜: {title}")
                        continue

                    if not has_detail:
                        # é¦–å…ˆæ£€æŸ¥itemä¸­æ˜¯å¦ç›´æ¥åŒ…å«urlå­—æ®µ
                        if 'url' in item:
                            links = item['url']
                        else:
                            item_str = json.dumps(item, ensure_ascii=False)
                            links, disks = extract_links(item_str)
                            if not links:
                                continue
                        # è°ƒç”¨extract_linksç¡®ä¿èƒ½è¯†åˆ«ç½‘ç›˜ç±»å‹å¹¶æ ¼å¼åŒ–é“¾æ¥
                        formatted_links, disks = extract_links(links)
                        if formatted_links:
                            yield from self.finalize_item_safe(title, formatted_links, response.url, disks)
                    else:
                        id_val = item.get('id') or item.get('slug') or item.get('uuid')
                        if id_val:
                            detail_url = f"https://{cfg.get('host')}/d/{id_val}"
                            headers = self.base_headers.copy()
                            headers['Referer'] = response.url
                            yield scrapy.Request(detail_url, headers=headers, callback=self.parse_detail,
                                                 meta=detail_meta, dont_filter=True)
            except Exception as e:
                self.logger.error(f"JSON è§£æå¤±è´¥: {e}")

        elif mode == 'regex_json':
            match = re.search(cfg['extract_regex'], response.text)
            if match:
                try:
                    data = json.loads(match.group(1).replace('\\/', '/'))
                    for item in data:
                        title = item.get(cfg.get('json_title', 'title'))
                        if not has_detail:
                            links, disks = extract_links(json.dumps(item))
                            yield from self.finalize_item_safe(title, links, response.url, disks)
                        else:
                            url_val = item.get(cfg.get('json_url', 'url'))
                            if url_val:
                                full_url = response.urljoin(url_val)
                                yield scrapy.Request(full_url, callback=self.parse_detail, meta=detail_meta,
                                                     dont_filter=True)
                except:
                    pass
        else:
            rules = cfg.get('list_rules', {})
            for node in response.xpath(rules.get('item_nodes', '')):
                title = node.xpath(rules.get('title_node', './/text()')).get()
                if not title: continue

                if not has_detail:
                    links, disks = extract_links(node.get())
                    if links:
                        yield from self.finalize_item_safe(title, links, response.url, disks)
                else:
                    link = node.xpath(rules.get('detail_link', '')).get()
                    if link:
                        full_url = response.urljoin(link)
                        headers = self.base_headers.copy()
                        headers['Referer'] = response.url
                        yield scrapy.Request(full_url, headers=headers, callback=self.parse_detail, meta=detail_meta,
                                             dont_filter=True)

    def parse_detail(self, response):
        if response.status == 403: return
        fields = self.site_cfg.get('detail_rules', {}).get('fields', {})
        title_raw = response.xpath(fields.get('title', '//title/text()')).getall()
        title = "".join(title_raw).strip()
        links, disks = extract_links(response.text)
        if links:
            yield from self.finalize_item_safe(title, links, response.url, disks)

    def finalize_item_safe(self, title, links, source_url, disks=None):
        # 1. æ¸…æ´—æ ‡é¢˜
        clean_title = html.unescape(re.sub(r'<[^>]+>', '', str(title or "æ— æ ‡é¢˜"))).strip()

        # 2. é“¾æ¥æ¸…æ´—å¹¶å»é‡ï¼ˆä¿æŒä¸ºåˆ—è¡¨ï¼‰
        if isinstance(links, str):
            raw_list = [l.strip() for l in links.split(',') if l.strip()]
        else:
            raw_list = [str(l).strip() for l in links if str(l).strip()]
        
        unique_links = list(dict.fromkeys(raw_list))

        # 3. éå†é“¾æ¥ï¼Œæ¯ä¸€æ¡é“¾æ¥ yield ä¸€ä¸ªç‹¬ç«‹çš„ item
        for link in unique_links:
            fingerprint = get_md5(link)
            if fingerprint in self.seen_resources:
                continue
            self.seen_resources.add(fingerprint)

            # é‡æ–°è¯†åˆ«å•æ¡é“¾æ¥çš„ç½‘ç›˜ç±»å‹ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
            # è¿™æ ·æ¯ä¸€æ¡æ•°æ®éƒ½èƒ½å‡†ç¡®å¯¹åº”å®ƒçš„ç½‘ç›˜ç±»å‹
            from .utils import extract_links as re_extract
            _, single_disk = re_extract(link)

            self.logger.info(f"âœ¨ å‘ç°èµ„æº: {clean_title[:20]}... | é“¾æ¥: {link[:30]}...")
            
            yield {
                'site_name': str(self.site_cfg.get('name')),
                'title': clean_title,
                'disk_type': str(single_disk or "æœªçŸ¥"),
                'resource_url': link,  # ç°åœ¨è¿™é‡Œåªæœ‰ä¸€ä¸ªå•ç‹¬çš„ URL
                'source_url': str(source_url)
            }

    def get_json_value(self, obj, path):
        if not path: return None
        try:
            for key in path.split('.'):
                if isinstance(obj, dict):
                    obj = obj.get(key)
                elif isinstance(obj, list) and key.isdigit():
                    obj = obj[int(key)]
                else:
                    return None
            return obj
        except:
            return None

    def render_template(self, text):
        for k, v in self.context.items():
            val = urllib.parse.quote(str(v)) if k == "keyword" else str(v)
            text = text.replace(f"{{{k}}}", val)
        return text


def run():
    with open('sites.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    print("--- èµ„æºé‡‡é›†å¼•æ“ v6.2 (Per-Site Settings Mode) ---")
    target = input("è¾“å…¥ç«™ç‚¹ key (è¾“å…¥ 'all' è¿è¡Œå…¨éƒ¨): ").strip()
    kw = input("æœç´¢è¯: ").strip()

    output_file = 'out.jsonl'
    if os.path.exists(output_file):
        os.remove(output_file)

    # 1. è¿™é‡Œåªæ”¾ã€çœŸæ­£çš„å…¨å±€åŸºç¡€é…ç½®ã€‘
    process = CrawlerProcess(settings={
        'LOG_LEVEL': 'INFO',
        'FEEDS': {
            output_file: {
                'format': 'jsonlines',
                'overwrite': True,
                'encoding': 'utf8'
            }
        },
        'COOKIES_ENABLED': True,
        'AUTOTHROTTLE_ENABLED': True,  # å¼€å¯è‡ªåŠ¨é™é€Ÿï¼Œé…åˆè‡ªå®šä¹‰å»¶è¿Ÿ
        'AUTOTHROTTLE_START_DELAY': 1.0,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'DOWNLOAD_TIMEOUT': 20,
    })

    def _crawl_site(site_key, site_cfg):
        # 2. å…³é”®ç‚¹ï¼šä¸ºæ¯ä¸ªç«™ç‚¹åŠ¨æ€æ„å»ºä¸ªæ€§åŒ–è®¾ç½®
        # è¿™äº›è®¾ç½®ä¼šè¦†ç›–ä¸Šé¢çš„å…¨å±€è®¾ç½®
        site_specific_settings = {
            # å¦‚æœ YAML æ²¡å†™ï¼Œåˆ™ç»™ä¸ªé»˜è®¤å€¼
            'CONCURRENT_REQUESTS_PER_DOMAIN': site_cfg.get('concurrent', 4),
            'DOWNLOAD_DELAY': site_cfg.get('delay', 1.0),
        }

        # 3. å°†è®¾ç½®æ³¨å…¥åˆ° crawl æ–¹æ³•ä¸­
        process.crawl(
            UniversalSpider,
            site_cfg=site_cfg,
            keyword=kw,
            # é€šè¿‡ settings å‚æ•°ä¼ é€’ï¼ŒScrapy ä¼šè‡ªåŠ¨åº”ç”¨
            settings=site_specific_settings
        )

    if target.lower() == 'all':
        for s_key, s_cfg in config['sites'].items():
            _crawl_site(s_key, s_cfg)
    elif target in config['sites']:
        _crawl_site(target, config['sites'][target])
    else:
        print("ç«™ç‚¹ä¸å­˜åœ¨ï¼")
        return

    process.start()


if __name__ == "__main__":
    run()